{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vggish.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPXzvhn1UFhIHAhUjkQs4fy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZvicDO9rEy_M"},"source":["### FOR COLAB\n","import os\n","is_colab = 'google.colab' in str(get_ipython())\n","if is_colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    colab_path = '/content/drive/MyDrive/HumbugDB new/HumBugDB/notebooks'\n","    if not os.getcwd().endswith('ish'):\n","      os.chdir(colab_path)\n","    ## DO VERSION CORRECTING STUFFX_test.shape\n","    \n","    # TODO\n","import sys\n","sys.path.insert(0, os.path.abspath('../torchvggish/torchvggish'))\n","sys.path.append('../lib')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU1dZnmoVy_S"},"source":["import torch\n","import matplotlib.pyplot as plt\n","from torch import nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import numpy as np\n","import config\n","from sklearn.utils import shuffle, class_weight\n","from sklearn.preprocessing import scale\n","from tqdm import tqdm\n","import datetime\n","import pickle\n","from vggish_input import wavfile_to_examples\n","from os.path import join\n","from evaluate import get_results, plot_confusion_matrix_multiclass, compute_plot_roc_multiclass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_YRgs5UBvJW"},"source":["model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n","model.eval()\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","# Download an example audio file\n","import urllib\n","url, filename = (\"http://soundbible.com/grab.php?id=1698&type=wav\", \"bus_chatter.wav\")\n","file_dir = '../torchvggish'\n","try: urllib.URLopener().retrieve(url, os.path.join(file_dir, filename))\n","except: urllib.request.urlretrieve(url, os.path.join(file_dir, filename))\n","\n","bus_chatter_out = model.forward(os.path.join(file_dir, filename))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0b6bYXfHIZW"},"source":["bus_chatter_out = model.forward(os.path.join(file_dir, filename))\n","bus_chatter_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xLuEWaJM86g"},"source":["log_mel_examples = wavfile_to_examples(join(file_dir, 'test.wav')) \n","bus_feat = wavfile_to_examples(join(file_dir, 'bus_chatter.wav')) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcZgoVSkPNK8"},"source":["print(log_mel_examples.shape, bus_feat.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEwrAWLfBe_m"},"source":["print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKyhDIlaV3HT"},"source":["# Dataframe stuff\n"]},{"cell_type":"code","metadata":{"id":"7lFiKDjTzUYM"},"source":["model.preprocess = False # we can't use the standard preprocessing as we need to work from clips"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCj5sfZ9WYZO"},"source":["df = pd.read_csv(config.data_df)\n","idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n","df_all = df[idx_multiclass]\n","classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n","               'an coustani','ma uniformis','ma africanus']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4w5K5rtlVnj"},"source":["# work with much smaller subset of data so this runs quicker - get rid of once it's working\n","# and replace with a pckle saver/loader\n","# df_all = df_all.iloc[::20,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixMVXpLq29L5"},"source":["# copied from species_classification\n","import collections\n","species_dict = collections.OrderedDict()\n","species_recordings = collections.OrderedDict()\n","for species in classes:\n","    # Number of total audio clips per species (includes repeats from same filename)\n","    species_recordings[species] = len(pd.unique(df_all[df_all.species==species].name)) # Number of unique audio recordings (and hence mosquitoes)\n","    species_dict[species] = sum(df_all[df_all.species==species].length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hCaClqy29L6"},"source":["# Divide recordings into train and test, with recording shuffling fixed by random_state\n","train_fraction = 0.75\n","train_recordings = {}\n","test_recordings = {}\n","\n","for i in range(len(classes)):\n","    n_train = int(species_recordings[classes[i]] * train_fraction)\n","    n_test = species_recordings[classes[i]] - n_train\n","    print(classes[i], n_train, n_test)\n","    df_class = df_all[df_all.species == classes[i]]\n","    # !!!I think it is probably best to shuffle these to mimic W but need to make sure each ID of mosquito is unique per recording\n","    train_recordings[i] =  shuffle(pd.unique(df_class.name), random_state=42)[:n_train]  \n","    test_recordings[i] = shuffle(pd.unique(df_class.name),random_state=42)[n_train:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M9n_azoeAnkI"},"source":["# Make and pickle features"]},{"cell_type":"code","metadata":{"id":"3Lh5FaR2PcS_"},"source":["def make_vggish_feats(df_all, label_recordings_dict, data_dir):\n","  X, y = [], []\n","  for class_label in tqdm(label_recordings_dict.keys(), position=0, leave=True): # Loop over classes\n","    for i in label_recordings_dict[class_label]: # Loop over recordings in class\n","      df_match = df_all[df_all.name == i]\n","      for idx, row in df_match.iterrows(): # Loop over clips in recording\n","        _, file_format = os.path.splitext(row['name'])\n","        filename = os.path.join(data_dir, str(row['id']) + file_format)\n","        feat = wavfile_to_examples(filename)\n","        # if config.norm_per_sample:\n","          # feat = (feat-np.mean(feat))/np.std(feat) # TODO come back to          \n","        X.append(feat)\n","        y.append(class_label)\n","  return X, y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUKarNZ7Wa8d"},"source":["# X_test, y_test = make_vggish_feats(df_all, test_recordings, config.data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYMojGLtk-TJ"},"source":["# X_train, y_train = make_vggish_feats(df_all, train_recordings, config.data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfO9QbXvkCLP"},"source":["# Now we need a reshaper to turn this into a form that the network can accept\n","# we have a list of n tensors and n labels, each tensor mx1x96x64\n","# we need to produce a single training tensor that is _x1x96x64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lCoeoyH70A1N"},"source":["def reshape(X, y):\n","  tmp = []\n","  for i in range(len(X)):\n","    tmp += [y[i]] * X[i].shape[0]\n","  y = np.array(tmp)\n","  X = torch.cat(X, dim=0)\n","  return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnWRY9f22eTa"},"source":["# X_train, y_train = reshape(X_train, y_train)\n","# X_test, y_test = reshape(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXjSyxyqZOYX"},"source":["def pickle_features():\n","  # Pickle features for whole dataset\n","  dt = datetime.datetime.now()\n","  seq = str(int(dt.strftime(\"%Y%m%d%H%M%S\")))\n","  pickle_dir='../outputs/features/vggish'\n","  if not os.path.exists(pickle_dir):\n","    os.mkdir(pickle_dir)\n","  test_pickle = 'test_feats_vggish_' + seq + '.pkl'\n","  train_pickle = 'train_feats_vggish_' + seq + '.pkl'\n","  with open(join(pickle_dir, train_pickle), 'wb') as f:\n","    pickle.dump([X_train, y_train], f, protocol=4)\n","  with open(join(pickle_dir, test_pickle), 'wb') as f:\n","    pickle.dump([X_test, y_test], f, protocol=4)\n","# pickle_features()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ybYOI9pNP2Kr"},"source":["# Load pickled features"]},{"cell_type":"code","metadata":{"id":"jLqG5mEzXeyG"},"source":["def load_vggish_feats(file, dir='../outputs/features/vggish'):\n","  if os.path.exists(join(dir, file)):\n","    with open(join(dir, file), 'rb') as f:\n","      [X, y] = pickle.load(f)\n","  return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2D2oruk4G8S"},"source":["test_file = 'test_feats_vggish_20210802124327.pkl'\n","train_file = 'train_feats_vggish_20210802124327.pkl'\n","X_train, y_train = load_vggish_feats(train_file)\n","X_test, y_test = load_vggish_feats(test_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sI4JA59X0EE0"},"source":["print(len(X_train), X_train[1].shape, len(y_train), type(y_train[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqpU70LvjJRc"},"source":["from runTorchMultiClass import train_model, test_model, load_model, evaluate_model\n","# test_X_train = test_feat_output # as in from test.wav not tes data\n","# test_y_train = np.random.randint(8, size=(4,)) # numpy array not torch tensor - a problem?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbtK1HuGimH0"},"source":["X_train_embed = model.forward(X_train[280:560])\n","alist = [X_train_embed]*100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIsCv_hzjBbH"},"source":["X_train_embed.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdAHTM6ajHE5"},"source":["model.preprocessing = False\n","X_train_embed = []\n","for i in range(101):\n","  chunk = [280*i, 280*(i+1)]\n","  print(chunk)\n","  X_train_embed.append(model.forward(X_train[chunk[0]:chunk[1]]))\n","len(X_train_embed)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixeAgc0Wpozp"},"source":["100*[X_train_embed]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PD-PKFFfnSDK"},"source":["model.preprocessing = False\n","X_train_embed = []\n","for i in range(6):\n","  chunk = [280*i, 280*(i+1)]\n","  print(chunk)\n","  X_train_embed.append(model.forward(X_train[chunk[0]:chunk[1]]))\n","len(X_train_embed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEAC9VPqnMiS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HW2UsQaWBG5w"},"source":["# Train model"]},{"cell_type":"code","metadata":{"id":"0fmjBy5TKfG1"},"source":["num_classes = 8 # in a config file or something\n","class VGGishMulticlass(nn.Module):\n","  def __init__(self, num_classes, vggish_model=None, dropout=0.2): # dropout?\n","    super(VGGishMulticlass, self).__init__()\n","    self.dropout = dropout\n","    # TODO go back to how it was before, so you can control preprocessing\n","    if vggish_model is not None:\n","      self.vggish_embedding = vggish_model\n","    else: \n","      self.vggish_embedding = torch.hub.load('harritaylor/torchvggish', 'vggish', pretrained=True)\n","    self.fc1 = nn.Linear(128, num_classes)\n","  def forward(self, x):\n","      x = self.vggish_embedding(x).squeeze()\n","      x = x/255   # normalise fc1 input between 0 and 1\n","      # x_mean, x_std = torch.mean(x, axis=1), torch.std(x, axis=1)\n","      # x = x - x_mean.reshape(-1,1)\n","      # x = x / x_std.reshape(-1,1)   # Z-normalise inputs to fc1\n","      \n","      # print(x.shape)\n","      x = self.fc1(F.dropout(x,p=self.dropout))\n","      # x = torch.sigmoid(x) \n","      # print(x[0])\n","      # x = F.relu(x)\n","      \n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwPk-NNlsbOY"},"source":["model = torch.hub.load('harritaylor/torchvggish', 'vggish', pretrained=True)\n","model.eval()\n","model.preprocess = False\n","print(model(X_train[:10]))\n","print(model.forward(X_train[:10]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7I7xhcOmdXfh"},"source":["# Testing class\n","x = VGGishMulticlass(num_classes)\n","x = x.to(device)\n","x.preprocess = True\n","# x.forward(join(file_dir, 'bus_chatter.wav'))[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZxwe7ySmqpj"},"source":["model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n","model.preprocess = False\n","model.postprocess = False\n","class_weights = class_weight.compute_class_weight('balanced',\n","                                                  classes=np.unique(y_train),\n","                                                  y=y_train)\n","trained_model = train_model(X_train, y_train, \n","                            class_weight=class_weights, \n","                            model=VGGishMulticlass(num_classes, model))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mKK-2Uxi73n"},"source":["sample_y_pred = [ 86.7832,  73.5336,  13.5915,  73.2701,  26.7167,  67.8825,  19.7151, 10.0495]\n","import scipy\n","print(scipy.special.softmax(sample_y_pred))\n","print(np.log(scipy.special.softmax(sample_y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0c5NCw0lNYK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRZsGkixqrqF"},"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGdsnIhEVWZ2"},"source":["trained_model.forward(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W19Brt8i2fq3"},"source":["evaluate_model(trained_model, X_test, y_test, n_samples=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fgpHhWWAeUHv"},"source":["# Experimenting/bug fixing"]},{"cell_type":"markdown","metadata":{"id":"-XlZUjlqQRtT"},"source":["## Plotting embeddings"]},{"cell_type":"code","metadata":{"id":"IvDlXHN2WQo6"},"source":["def plot_embeddings(tensor, labels):\n","  plt.figure(figsize=[25,8])\n","  plt.plot(tensor.detach().numpy().T)\n","  labels = labels.tolist()\n","  for i in range(len(labels)):\n","    labels[i]= str(labels[i])\n","  print(labels)\n","  plt.legend(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9ZY0df5OMU8"},"source":["# plot embedded features from standard model\n","model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n","model.postprocess = False\n","model.preprocess = False\n","random_sample = np.random.randint(X_test.shape[0], size=(5,))\n","print(random_sample)\n","X_test_fraction = X_test[random_sample]\n","y_labels_sample = y_test[random_sample]\n","# X_test_fraction = X_test[np.arange(4)*3000]\n","output = model.forward(X_test_fraction).cpu()\n","plot_embeddings(output, y_labels_sample)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekv4Hz0bfYov"},"source":["X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPGHreuiST2B"},"source":["# compare with features from bus_chatter.wav\n","plot_embeddings(bus_chatter_out[:5].cpu())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZPinUZoZT9a"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ca-SdqweFBu"},"source":["## Checking values before going into loss function/softmax"]},{"cell_type":"markdown","metadata":{"id":"yoG-l8YsePaT"},"source":[""]}]}